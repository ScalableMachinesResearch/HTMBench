CLOMP_TM 1.6 was written at LLNL by John Gyllenhaal between February 2009 
and November 2012 based on numerous detailed discussions with 
Greg Bronevetsky (on the original CLOMP), Barna Bihari, and Martin Schindewolf
at LLNL and Amy Wang at IBM.   

CLOMP_TM is a highly parameterized synthetic benchmark that has been used to
study transactional memory and speculative execution on BG/Q systems and
OpenMP overheads on a variety of systems.  In addition to measuring
performance, CLOMP_TM has also been designed to detect a variety of
correctness issues and will abort if any wrong answers are detected.

It was used extensively in the SC12 paper "What Scientific Applications can
Benefit from Hardware Transactional Memory?" by M. Schindewolf, B. Bihari,
J. Gyllenhaal, M.  Schulz, A. Wang, W. Karl.   

A technical report containing much of the same material can be found here: 
https://e-reports-ext.llnl.gov/pdf/621619.pdf.   

Please see the SC12 paper or this technical report for examples of what 
experiments can be done with the CLOMP_TM benchmark.  

-------------------------------------
Here is the output from building CLOMP_TM 1.6 on BG/Q with the 
provided Makefile:

>>make bgq
mpixlc_r -DBGQ -DDIVIDE_CALC -DCALC_STRIDE=4  -qsmp=speculative:noauto -qtm -qarch=qp -qtune=qp -q64 -qcpluscmt -O3 -qstrict clomp_tm.c -o clomp_tm_bgq_divide4 -lm 
mpixlc_r -DBGQ -DSE -DDIVIDE_CALC -DCALC_STRIDE=4  -qsmp=speculative:noauto -qsuppress=1506-1463 -qtm -qarch=qp -qtune=qp -q64 -qcpluscmt -O3 -qstrict clomp_tm.c -o clomp_se_bgq_divide4 -lm 
=> Compile of clomp_tm completed.
\n*** RECOMMEND SETTING THE FOLLOWING TO GET GOOD TIMINGS AND STATS
setenv OMP_WAIT_POLICY ACTIVE
setenv BG_SMP_FAST_WAKEUP YES
setenv OMP_NUM_THREADS (number)
setenv TM_REPORT_STAT_ENABLE YES
setenv SE_REPORT_STAT_ENABLE YES


-------------------------------------
The BG/Q environment variables suggested are required for good performance, 
so we always set them before running:

>> setenv OMP_WAIT_POLICY ACTIVE
>> setenv BG_SMP_FAST_WAKEUP YES
>> setenv OMP_NUM_THREAD 64
>> setenv TM_REPORT_STAT_ENABLE YES
>> setenv SE_REPORT_STAT_ENABLE YES

-------------------------------------
Here is the 'help output (we use srun at LLNL, mpirun is the standard 
launch command on other bgq systems): 

>> srun -n 1 -ppdebug ./clomp_tm_bgq_divide4
Usage: clomp_tm_bgq_divide4 numThreads allocThreads numParts zonesPerPart zoneSize \
          scatterMode[%mainMod][,altScatterCount,altScatterMode[%altMod]] \
          scatter flopScale randomSeed scrubRate timeScale

  numThreads: Number of OpenMP threads to use (-1 for system default)
  allocThreads: #threads when allocating data (-1 for numThreads)
  numParts: Number of independent pieces of work (loop iterations)
            xN sets numParts to 'N * numThreads' (N >= 1)
  zonesPerPart: Number of zones in each part. (>= 1)
                dM sets zonesPerPart to 'M / numParts' (d6144 nominal)
  zoneSize: Bytes in zone, 1 calc for every 64 bytes (power of 2 >= 32 valid)
  flopScale: Scales flops/zone to increase memory reuse (1 nominal, >=1 Valid)
  randomSeed: Added to initial random seed (0 nominal, >=0 Valid)
  scrubRate: Hardware SpecId scrub rate (66 default, 6 recommended)
  timeScale: Scales target time per test (10-100 nominal, 1-100000 Valid)
  scatterMode: None (no races), Stride1 (no races), Adjacent, firstParts,
               Random, InPart (no races), firstZone (no races), randFirstZone
  NOTE: ALL scatterModes, even Random, produces the same layout every run.
  mainMod: Applies % mainMod to partId when doing scatterMode (more overlap)
           /D sets mainMod to 'numParts / D'
  altScatterCount: # of zones selected via altScatterMode (>=1 up to 90% of total)
  altScatterMode: Used instead of scatterMode for randomly selected updates
  altMod: Applies % altMod to partId when doing altScatterMode (more overlap)
          /D sets altMod to 'numParts / D'
  scatter: Number of zones updated when 'updating' a zone  (>=1 Valid)

Some interesting testcases (last number controls run time, use <=10 for simulator):

Ten Atomic versus One TM (or using SE) testcases:
       No conflicts:  clomp_tm_bgq_divide4 -1 -1 x8 d32768 256 Stride1 10 1 0 6 1000
Very Rare conflicts:  clomp_tm_bgq_divide4 -1 -1 x8 d32768 256 Stride1,8,Adjacent 10 1 0 6 1000

Prefetch-friendly testcases:
Very Rare conflicts:  clomp_tm_bgq_divide4 -1 1 x4 d6144 128 None,10,Adjacent 3 1 0 6 1000
     Rare conflicts:  clomp_tm_bgq_divide4 -1 1 x4 d6144 128 Adjacent 3 1 0 6 1000
     High conflicts:  clomp_tm_bgq_divide4 -1 1 64 100 128 firstParts 3 1 0 6 1000
       No conflicts:  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 Stride1 3 1 0 6 1000

TM Rand emulations (set altScatterCount to 6144 * 3 * target_percentage):
       TM Rand (1%):  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 firstZone,184,randFirstZone 3 1 0 6 1000
      TM Rand (50%):  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 firstZone,9216,randFirstZone 3 1 0 6 1000
      TM Rand (99%):  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 randFirstZone,184,firstZone 3 1 0 6 1000
                      Note: Need to flip scatter modes for >= 90% altScatterCount

TM Rand plus varation where only two threads can hit same memory location:
  TM Rand plus (1%):  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 firstZone,184,firstZone%/2 3 1 0 6 1000
  TM Rand plus (50%):  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 firstZone,9216,firstZone%/2 3 1 0 6 1000
  TM Rand plus (99%):  clomp_tm_bgq_divide4 -1 1 x1 d6144 128 firstZone%/2,184,firstZone 3 1 0 6 1000

Prefetch-unfriendly testcases:
Very Rare conflicts:  clomp_tm_bgq_divide4 -1 1 64 100 128 InPart,10,Random 10 1 0 6 1000
     Rare conflicts:  clomp_tm_bgq_divide4 -1 1 64 100 128 Random 10 1 0 6 1000
Huge atomic regions:  clomp_tm_bgq_divide4 -1 1 64 10 128 Random 100 1 0 6 1000
       No conflicts:  clomp_tm_bgq_divide4 -1 1 64 100 128 InPart 10 1 0 6 1000


-------------------------------------
Here is the output of a short run similar to one interesting run from
the paper:

>> srun -n1 clomp_tm_bgq_divide4 -1 -1 x8 d32768 256 Stride1 10 1 0 6 100
CLOMP_TM Version 1.60 (9Nov2012).Personality: (0 of 4, 1 of 4, 0 of 4, 0 of 4, 0 of 2, 0 of 4), R00-M0-N00-J03-C00
       Invocation: ./clomp_tm_bgq_divide4 -1 -1 x8 d32768 256 Stride1 10 1 0 6 100
         Hostname: rzuseqio2-ib0
       Start time: Fri Nov  9 17:08:36 2012
       Executable: ./clomp_tm_bgq_divide4
      numThreads: 64 (using system default)
    allocThreads: 64 (using numThreads)
        numParts: 512
    zonesPerPart: 64
        zoneSize: 256
  zone alignment: 256
     scatterMode: Stride1
         scatter: 10
       flopScale: 1
      randomSeed: 0
       scrubRate: 6
       timeScale: 100
   Zones per Part: 64
      Total Zones: 32768
Extra Zone Values: 25
 Calc Start Index: 1
 Zone Calc Stride: 4
 Extra Zone Calcs: 6
   Zone Calc Flag: -DDIVIDE_CALC
Zone Calc Formula: ((1.0/(x+2.0))-0.5)
     SIMD Enabled: No (calc's dependent)
SelfScrub SpecIds: Disabled (default)
Memory (in bytes): 8429568
Scaled Iterations: 4
  Total Subcycles: 40
 Small TM Updates: 13107200
 Large TM Updates: 1310720
Iteration Residue: 14.076864%
  Max Error bound: 6.2393577e-07
Tight Error bound: 2.389716e-09
      Max Residue: 0.16383089
---------------------
 calc_deposit:| ------ Start calc_deposit Pseudocode ------
 calc_deposit:| /* Measure *only* non-threadable calc_deposit() overhead.*/
 calc_deposit:| /* Expect this overhead to be negligible.*/
 calc_deposit:| deposit = calc_deposit ();
 calc_deposit:| ------- End calc_deposit Pseudocode -------
 calc_deposit  Started: Fri Nov  9 17:08:36 2012
 calc_deposit  Runtime: 5.79357e-05 (wallclock, in seconds)
 calc_deposit  us/Loop: 1.44839 (wallclock, in microseconds)
---------------------
  OMP Barrier:| ------ Start OMP Barrier Pseudocode ------
  OMP Barrier:| /* Measure *only* OMP barrier overhead.*/
  OMP Barrier:| #pragma omp barrier
  OMP Barrier:| ------- End OMP Barrier Pseudocode -------
  OMP Barrier  Started: Fri Nov  9 17:08:36 2012
  OMP Barrier #Threads: 64
  OMP Barrier  Runtime: 5.00679e-05 (wallclock, in seconds)
  OMP Barrier  us/Loop: 1.2517 (wallclock, in microseconds)
---------------------
   Serial Ref:| ------ Start Serial Ref Pseudocode ------
   Serial Ref:| /* Measure serial reference performance */
   Serial Ref:| deposit = calc_deposit ();
   Serial Ref:| for (pidx = 0; pidx < numParts; pidx++)
   Serial Ref:|   update_part_no_TM (partArray[pidx], deposit);
   Serial Ref:| ------- End Serial Ref Pseudocode -------
   Serial Ref  Started: Fri Nov  9 17:08:37 2012
   Serial Ref #Threads: N/A
   Serial Ref Checksum: Sum=39.836169 Residue=0.16383089 Total=40
   Serial Ref  Runtime: 5.14577 (wallclock, in seconds)
   Serial Ref  us/Loop: 128644 (wallclock, in microseconds)
---------------------
     Bestcase:| ------ Start Bestcase Pseudocode ------
     Bestcase:| /* Use OpenMP parallel for schedule(static) on original loop. */
     Bestcase:| deposit = calc_deposit ();
     Bestcase:| #pragma omp parallel for private (pidx) schedule(static)
     Bestcase:| for (pidx = 0; pidx < numParts; pidx++)
     Bestcase:|   update_part_no_TM (partArray[pidx], deposit);
     Bestcase:| ------- End Bestcase Pseudocode -------
     Bestcase  Started: Fri Nov  9 17:08:43 2012
     Bestcase #Threads: 64
     Bestcase  Runtime: 0.0994658 (wallclock, in seconds)
     Bestcase  us/Loop: 2486.65 (wallclock, in microseconds)
     Bestcase  Speedup: 51.73
---------------------
     Small TM:| ------ Start Small TM Pseudocode ------
     Small TM:| /* Use OpenMP parallel for schedule(static) on original loop. */
     Small TM:| deposit = calc_deposit ();
     Small TM:| #pragma omp parallel for private (pidx) schedule(static)
     Small TM:| for (pidx = 0; pidx < numParts; pidx++)
     Small TM:|   update_part_small_TM (partArray[pidx], deposit);
     Small TM:| ------- End Small TM Pseudocode -------
     Small TM  Started: Fri Nov  9 17:08:43 2012
     Small TM #Threads: 64
     Small TM Checksum: Sum=39.836169 Residue=0.16383089 Total=40
     Small TM  TM/Loop: 327680 (reported by tm_get_all_stats)
     Small TM  RB/Loop: 0.00 (average rollbacks/loop)
     Small TM    RB/TM: 0.00% (percent rollbacks/TM)
     Small TM Total RB: 0 (total rollbacks)
     Small TM  Runtime: 0.645914 (wallclock, in seconds)
     Small TM  us/Loop: 16147.8 (wallclock, in microseconds)
     Small TM  Speedup: 7.97
     Small TM Efficacy: 15.40% (of bestcase 2486.65 us/Loop)
     Small TM Overhead: 13661.2 (versus bestcase, in us/Loop)
     Small TM OverH/TM: 0.0416907 (TM Overhead, in us/transaction)
---------------------
     Large TM:| ------ Start Large TM Pseudocode ------
     Large TM:| /* Use OpenMP parallel for schedule(static) on original loop. */
     Large TM:| deposit = calc_deposit ();
     Large TM:| #pragma omp parallel for private (pidx) schedule(static)
     Large TM:| for (pidx = 0; pidx < numParts; pidx++)
     Large TM:|   update_part_large_TM (partArray[pidx], deposit);
     Large TM:| ------- End Large TM Pseudocode -------
     Large TM  Started: Fri Nov  9 17:08:44 2012
     Large TM #Threads: 64
     Large TM Checksum: Sum=39.836169 Residue=0.16383089 Total=40
     Large TM  TM/Loop: 32768 (reported by tm_get_all_stats)
     Large TM  RB/Loop: 1.32 (average rollbacks/loop)
     Large TM    RB/TM: 0.00% (percent rollbacks/TM)
     Large TM Total RB: 53 (total rollbacks)
     Large TM  Runtime: 0.141794 (wallclock, in seconds)
     Large TM  us/Loop: 3544.86 (wallclock, in microseconds)
     Large TM  Speedup: 36.29
     Large TM Efficacy: 70.15% (of bestcase 2486.65 us/Loop)
     Large TM Overhead: 1058.21 (versus bestcase, in us/Loop)
     Large TM OverH/TM: 0.032294 (TM Overhead, in us/transaction)
---------------------
 Small Atomic:| ------ Start Small Atomic Pseudocode ------
 Small Atomic:| /* Use OpenMP parallel for schedule(static) on original loop. */
 Small Atomic:| deposit = calc_deposit ();
 Small Atomic:| #pragma omp parallel for private (pidx) schedule(static)
 Small Atomic:| for (pidx = 0; pidx < numParts; pidx++)
 Small Atomic:|   update_part_small_atomic (partArray[pidx], deposit);
 Small Atomic:| ------- End Small Atomic Pseudocode -------
 Small Atomic  Started: Fri Nov  9 17:08:45 2012
 Small Atomic #Threads: 64
 Small Atomic Checksum: Sum=39.836169 Residue=0.16383089 Total=40
 Small Atomic  Runtime: 0.164711 (wallclock, in seconds)
 Small Atomic  us/Loop: 4117.77 (wallclock, in microseconds)
 Small Atomic  Speedup: 31.24
 Small Atomic Efficacy: 60.39% (of bestcase 2486.65 us/Loop)
 Small Atomic Overhead: 1631.13 (versus bestcase, in us/Loop)
---------------------
SmallCritical:| ------ Start Small Critical Pseudocode ------
SmallCritical:| /* Use OpenMP parallel for schedule(static) on original loop. */
SmallCritical:| deposit = calc_deposit ();
SmallCritical:| #pragma omp parallel for private (pidx) schedule(static)
SmallCritical:| for (pidx = 0; pidx < numParts; pidx++)
SmallCritical:|   update_part_small_critical (partArray[pidx], deposit);
SmallCritical:| ------- End Small Critical Pseudocode -------
SmallCritical  Started: Fri Nov  9 17:08:46 2012
SmallCritical #Threads: 64
SmallCritical Checksum: Sum=39.836169 Residue=0.16383089 Total=40
SmallCritical  Runtime: 4.68668 (wallclock, in seconds)
SmallCritical  us/Loop: 117167 (wallclock, in microseconds)
SmallCritical  Speedup: 1.10
SmallCritical Efficacy: 2.12% (of bestcase 2486.65 us/Loop)
SmallCritical Overhead: 114680 (versus bestcase, in us/Loop)
---------------------
LargeCritical:| ------ Start Large Critical Pseudocode ------
LargeCritical:| /* Use OpenMP parallel for schedule(static) on original loop. */
LargeCritical:| deposit = calc_deposit ();
LargeCritical:| #pragma omp parallel for private (pidx) schedule(static)
LargeCritical:| for (pidx = 0; pidx < numParts; pidx++)
LargeCritical:|   update_part_large_critical (partArray[pidx], deposit);
LargeCritical:| ------- End Large Critical Pseudocode -------
LargeCritical  Started: Fri Nov  9 17:08:53 2012
LargeCritical #Threads: 64
LargeCritical Checksum: Sum=39.836169 Residue=0.16383089 Total=40
LargeCritical  Runtime: 6.07837 (wallclock, in seconds)
LargeCritical  us/Loop: 151959 (wallclock, in microseconds)
LargeCritical  Speedup: 0.85 (1.18X slowdown)
LargeCritical Efficacy: 1.64% (of bestcase 2486.65 us/Loop)
LargeCritical Overhead: 149473 (versus bestcase, in us/Loop)
---------------------

Here is the copyright and license for CLOMP_TM 1.6:

******************************************************************************
COPYRIGHT AND LICENSE


Copyright (c) 2012, Lawrence Livermore National Security, LLC.
Produced at the Lawrence Livermore National Laboratory
Written by John Gyllenhaal (gyllen@llnl.gov)
LLNL-CODE-582532
OCEC-12-058
All rights reserved.

Redistribution and use in source and binary forms, with or without 
modification, are permitted provided that the following conditions are met:

o  Redistributions of source code must retain the above copyright notice, 
   this list of conditions and the disclaimer below.

o  Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the disclaimer (as noted below) in the 
   documentation and/or other materials provided with the distribution.

o  Neither the name of the LLNS/LLNL nor the names of its contributors may 
   be used to endorse or promote products derived from this software without
   specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" 
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, 
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL LAWRENCE LIVERMORE NATIONAL
SECURITY, LLC, THE U.S. DEPARTMENT OF ENERGY OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
 USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

Additional BSD Notice

 1. This notice is required to be provided under our contract with the
    U.S. Department of Energy (DOE). This work was produced at
    Lawrence Livermore National Laboratory under Contract 
    No. DE-AC52-07NA27344 with the DOE.

 2. Neither the United States Government nor Lawrence Livermore National
    Security, LLC nor any of their employees, makes any warranty, express
    or implied, or assumes any liability or responsibility for the accuracy,
    completeness, or usefulness of any information, apparatus, product, or
    process disclosed, or represents that its use would not infringe
    privately-owned rights.

3. Also, reference herein to any specific commercial products, process, or
   services by trade name, trademark, manufacturer or otherwise does not
   necessarily constitute or imply its endorsement, recommendation, or 
   favoring by the United States Government or Lawrence Livermore National
   Security, LLC. The views and opinions of authors expressed herein do not
   necessarily state or reflect those of the United States Government or
   Lawrence Livermore National Security, LLC, and shall not be used for
   advertising or product endorsement purposes.
******************************************************************************/

